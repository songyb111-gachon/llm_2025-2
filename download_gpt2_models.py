"""
GPT-2 ê³„ì—´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
ê³¼ì œì— í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ìºì‹œì— ì €ì¥
"""

import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from tqdm import tqdm

# ê³¼ì œìš© GPT-2 ê³„ì—´ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
GPT2_MODELS = {
    "distilgpt2": {
        "name": "distilbert/distilgpt2",
        "size": "88M",
        "memory": "~4GB",
        "description": "DistilGPT-2 (ê°€ì¥ ì‘ìŒ)"
    },
    "gpt2": {
        "name": "openai-community/gpt2",
        "size": "0.1B (124M)",
        "memory": "~4GB",
        "description": "GPT-2 (base)"
    },
    "gpt2-large": {
        "name": "openai-community/gpt2-large",
        "size": "0.8B (774M)",
        "memory": "~8GB",
        "description": "GPT-2 Large"
    },
    "gpt2-xl": {
        "name": "openai-community/gpt2-xl",
        "size": "2B (1.5B)",
        "memory": "~16GB@training, ~36GB@full training",
        "description": "GPT-2 XL (ê°€ì¥ í¼)"
    }
}

def download_model(model_name: str, cache_dir: str = None):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œ
    
    Args:
        model_name: HuggingFace ëª¨ë¸ ì´ë¦„
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ ìœ„ì¹˜)
    """
    print(f"\n{'='*70}")
    print(f"ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name}")
    print(f"{'='*70}")
    
    try:
        # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        print("\n[1/2] í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
        )
        print(f"âœ“ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        print(f"  - Vocab size: {len(tokenizer)}")
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print("\n[2/2] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            cache_dir=cache_dir,
            low_cpu_mem_usage=True,
        )
        print(f"âœ“ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        num_params = sum(p.numel() for p in model.parameters())
        print(f"  - íŒŒë¼ë¯¸í„° ìˆ˜: {num_params:,} ({num_params/1e9:.2f}B)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        del tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"\nâœ… {model_name} ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"\nâŒ {model_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False


def download_multiple_models(model_keys, cache_dir=None):
    """ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"""
    
    models_to_download = []
    for key in model_keys:
        if key in GPT2_MODELS:
            models_to_download.append((key, GPT2_MODELS[key]))
        else:
            print(f"âš ï¸  ê²½ê³ : '{key}'ëŠ” ìœ íš¨í•œ ëª¨ë¸ í‚¤ê°€ ì•„ë‹™ë‹ˆë‹¤.")
    
    if not models_to_download:
        print("ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n{'='*70}")
    print(f"ì´ {len(models_to_download)}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print(f"{'='*70}")
    
    for key, info in models_to_download:
        print(f"\nëª¨ë¸: {info['description']}")
        print(f"í¬ê¸°: {info['size']}")
        print(f"ë©”ëª¨ë¦¬: {info['memory']}")
    
    success_models = []
    failed_models = []
    
    for i, (key, info) in enumerate(models_to_download, 1):
        print(f"\n\nì§„í–‰: [{i}/{len(models_to_download)}]")
        success = download_model(info['name'], cache_dir)
        
        if success:
            success_models.append(info['name'])
        else:
            failed_models.append(info['name'])
    
    # ìµœì¢… ê²°ê³¼
    print(f"\n\n{'='*70}")
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print(f"{'='*70}")
    print(f"\nâœ… ì„±ê³µ: {len(success_models)}ê°œ")
    for model in success_models:
        print(f"  - {model}")
    
    if failed_models:
        print(f"\nâŒ ì‹¤íŒ¨: {len(failed_models)}ê°œ")
        for model in failed_models:
            print(f"  - {model}")
    
    print(f"\n{'='*70}")


def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥"""
    print("\n" + "="*70)
    print("GPT-2 ê³„ì—´ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸")
    print("="*70 + "\n")
    
    for key, info in GPT2_MODELS.items():
        print(f"ğŸ“¦ [{key}] {info['description']}")
        print(f"   ëª¨ë¸: {info['name']}")
        print(f"   í¬ê¸°: {info['size']}")
        print(f"   ë©”ëª¨ë¦¬: {info['memory']}")
        print()
    
    print("="*70)
    print("\nì‚¬ìš©ë²•:")
    print("  # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    print("  python download_gpt2_models.py --all")
    print("\n  # íŠ¹ì • ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ")
    print("  python download_gpt2_models.py --models distilgpt2 gpt2")
    print("\n  # ì‘ì€ ëª¨ë¸ë§Œ (distilgpt2, gpt2, gpt2-large)")
    print("  python download_gpt2_models.py --small-only")
    print("="*70 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="GPT-2 ê³„ì—´ ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ëª¨ë“  GPT-2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì¶”ì²œ!) â­
  python download_gpt2_models.py --all
  
  # 16GB ì´í•˜ ì‘ì€ ëª¨ë¸ë§Œ (distilgpt2, gpt2, gpt2-large)
  python download_gpt2_models.py --small-only
  
  # íŠ¹ì • ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
  python download_gpt2_models.py --models distilgpt2 gpt2
  
  # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸
  python download_gpt2_models.py --list
  
  # ìºì‹œ ë””ë ‰í† ë¦¬ ì§€ì •
  python download_gpt2_models.py --all --cache_dir /path/to/cache
        """
    )
    
    parser.add_argument(
        "--all",
        action="store_true",
        help="ëª¨ë“  GPT-2 ê³„ì—´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"
    )
    parser.add_argument(
        "--small-only",
        action="store_true",
        help="16GB ì´í•˜ ì‘ì€ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ (distilgpt2, gpt2, gpt2-large)"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        choices=list(GPT2_MODELS.keys()),
        help="ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ í‚¤ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ~/.cache/huggingface)"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    if args.list:
        list_models()
        return
    
    # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ê²°ì •
    models_to_download = []
    
    if args.all:
        models_to_download = list(GPT2_MODELS.keys())
        print(f"\nğŸ“¦ ëª¨ë“  GPT-2 ëª¨ë¸ ({len(models_to_download)}ê°œ) ë‹¤ìš´ë¡œë“œ")
    elif args.small_only:
        models_to_download = ["distilgpt2", "gpt2", "gpt2-large"]
        print(f"\nğŸ“¦ ì‘ì€ GPT-2 ëª¨ë¸ ({len(models_to_download)}ê°œ) ë‹¤ìš´ë¡œë“œ")
    elif args.models:
        models_to_download = args.models
    else:
        print("ì—ëŸ¬: --all, --small-only ë˜ëŠ” --modelsë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        print("ë„ì›€ë§: python download_gpt2_models.py --help")
        print("\në˜ëŠ” ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸: python download_gpt2_models.py --list")
        return
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    if args.cache_dir:
        print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {args.cache_dir}")
    
    download_multiple_models(models_to_download, args.cache_dir)


if __name__ == "__main__":
    main()

