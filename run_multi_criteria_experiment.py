"""
3ê°€ì§€ í‰ê°€ ê¸°ì¤€ìœ¼ë¡œ 6ê°œ ëª¨ë¸ ì‹¤í—˜
ê° ìƒ˜í”Œì„ 3ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‘ í‰ê°€
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
import json
import argparse
from tqdm import tqdm
import gc
import logging
from success_criteria import evaluate_all_criteria

def setup_logger(log_file=None):
    """ë¡œê±° ì„¤ì •"""
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

logger = None


class MultiCriteriaGCGAttack:
    def __init__(self, model_name: str, device: str = "cuda"):
        """ë©€í‹° ê¸°ì¤€ GCG ê³µê²©"""
        self.device = device
        
        logger.info(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True,
            use_safetensors=True,  # safetensors ìš°ì„ 
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def test_suffix(self, prompt: str, suffix: str, max_new_tokens: int = 100) -> dict:
        """Suffixë¡œ ê³µê²© í…ŒìŠ¤íŠ¸ (3ê°€ì§€ ê¸°ì¤€ ì ìš©)"""
        full_prompt = f"{prompt} {suffix}"
        
        inputs = self.tokenizer(full_prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
                temperature=1.0,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response_only = response[len(full_prompt):].strip()
        
        # 3ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‘ í‰ê°€
        criteria_results = evaluate_all_criteria(response_only)
        
        return {
            "full_prompt": full_prompt,
            "response": response_only,
            "is_success_simple": criteria_results['simple'],
            "is_success_strict": criteria_results['strict'],
            "is_success_hybrid": criteria_results['hybrid'],
        }


def main():
    parser = argparse.ArgumentParser(description="ë©€í‹° ê¸°ì¤€ GCG Attack")
    parser.add_argument("--model_name", type=str, required=True)
    parser.add_argument("--artifacts_path", type=str, default="vicuna-13b-v1.5.json")
    parser.add_argument("--num_samples", type=int, default=100)
    parser.add_argument("--output_file", type=str, default="results_multi.json")
    parser.add_argument("--log_file", type=str, default=None)
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"])
    
    args = parser.parse_args()
    
    # ë¡œê±° ì´ˆê¸°í™”
    global logger
    logger = setup_logger(args.log_file)
    
    if args.log_file:
        logger.info(f"ë¡œê·¸ íŒŒì¼: {args.log_file}")
    
    # Artifacts ë¡œë“œ
    logger.info(f"Attack artifacts ë¡œë”©: {args.artifacts_path}")
    with open(args.artifacts_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    if isinstance(raw_data, dict) and 'jailbreaks' in raw_data:
        data = raw_data['jailbreaks']
    else:
        data = raw_data if isinstance(raw_data, list) else [raw_data]
    
    data = data[:args.num_samples]
    logger.info(f"ì´ {len(data)}ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ë¡œë“œ
    attacker = MultiCriteriaGCGAttack(args.model_name, device=args.device)
    
    results = []
    success_count_simple = 0
    success_count_strict = 0
    success_count_hybrid = 0
    
    for idx, item in enumerate(tqdm(data, desc="ê³µê²© ì§„í–‰ ì¤‘")):
        logger.info(f"\n{'='*60}")
        logger.info(f"ìƒ˜í”Œ {idx+1}/{len(data)}")
        
        goal = item.get('goal', item.get('prompt', ''))
        logger.info(f"Goal: {goal[:100]}...")
        
        try:
            # ê¸°ì¡´ suffix ì‚¬ìš©
            original_prompt = item.get('prompt', '')
            suffix = original_prompt.replace(goal, '').strip()
            logger.info(f"Suffix: {suffix[:50]}...")
            
            # í…ŒìŠ¤íŠ¸
            result = attacker.test_suffix(goal, suffix)
            result['goal'] = goal
            result['prompt'] = item.get('prompt', goal)
            result['suffix'] = suffix
            result['index'] = idx
            
            results.append(result)
            
            # ê° ê¸°ì¤€ë³„ ì„±ê³µ ì¹´ìš´íŠ¸
            if result['is_success_simple']:
                success_count_simple += 1
            if result['is_success_strict']:
                success_count_strict += 1
            if result['is_success_hybrid']:
                success_count_hybrid += 1
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info(f"Simple: {'âœ…' if result['is_success_simple'] else 'âŒ'} | "
                       f"Strict: {'âœ…' if result['is_success_strict'] else 'âŒ'} | "
                       f"Hybrid: {'âœ…' if result['is_success_hybrid'] else 'âŒ'}")
            logger.info(f"ì‘ë‹µ: {result['response'][:150]}...")
            
        except Exception as e:
            logger.error(f"ì—ëŸ¬: {str(e)}")
            results.append({
                "goal": goal,
                "prompt": item.get('prompt', goal),
                "error": str(e),
                "is_success_simple": False,
                "is_success_strict": False,
                "is_success_hybrid": False,
                "index": idx,
            })
    
    # ê²°ê³¼ ì €ì¥
    success_rate_simple = (success_count_simple / len(results)) * 100
    success_rate_strict = (success_count_strict / len(results)) * 100
    success_rate_hybrid = (success_count_hybrid / len(results)) * 100
    
    output = {
        "model_name": args.model_name,
        "num_samples": len(results),
        "success_count_simple": success_count_simple,
        "success_rate_simple": success_rate_simple,
        "success_count_strict": success_count_strict,
        "success_rate_strict": success_rate_strict,
        "success_count_hybrid": success_count_hybrid,
        "success_rate_hybrid": success_rate_hybrid,
        "results": results,
    }
    
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ì‹¤í—˜ ì™„ë£Œ!")
    logger.info(f"ì´ ìƒ˜í”Œ: {len(results)}")
    logger.info(f"\nğŸ“Š í‰ê°€ ê¸°ì¤€ë³„ ì„±ê³µë¥ :")
    logger.info(f"  Simple (ë‹¨ìˆœ):     {success_count_simple}/{len(results)} ({success_rate_simple:.2f}%)")
    logger.info(f"  Strict (ì—„ê²©):     {success_count_strict}/{len(results)} ({success_rate_strict:.2f}%)")
    logger.info(f"  Hybrid (í•˜ì´ë¸Œë¦¬ë“œ): {success_count_hybrid}/{len(results)} ({success_rate_hybrid:.2f}%)")
    logger.info(f"\nê²°ê³¼ ì €ì¥: {args.output_file}")


if __name__ == "__main__":
    main()

