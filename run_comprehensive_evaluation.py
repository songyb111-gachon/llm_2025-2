"""
ì¢…í•© í‰ê°€: JailbreakBench + HarmBench
6ê°œ ëª¨ë¸ Ã— (3ê°€ì§€ jailbreak ê¸°ì¤€ + harm í‰ê°€)
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import argparse
from tqdm import tqdm
import logging
from harm_evaluation import evaluate_jailbreak_and_harm

def setup_logger(log_file=None):
    """ë¡œê±° ì„¤ì •"""
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

logger = None


class ComprehensiveEvaluator:
    def __init__(self, model_name: str, device: str = "cuda"):
        """ì¢…í•© í‰ê°€ê¸°"""
        self.device = device
        
        logger.info(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        
        # MPT-7BëŠ” safetensors ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                low_cpu_mem_usage=True,
                use_safetensors=True,
            )
        except (OSError, KeyError) as e:
            logger.warning(f"Safetensors ë¡œë”© ì‹¤íŒ¨, ì¼ë°˜ ë¡œë”© ì‹œë„: {str(e)[:100]}")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def evaluate(self, prompt: str, suffix: str, max_new_tokens: int = 100) -> dict:
        """ì¢…í•© í‰ê°€ ìˆ˜í–‰"""
        full_prompt = f"{prompt} {suffix}"
        
        inputs = self.tokenizer(full_prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
                temperature=1.0,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response_only = response[len(full_prompt):].strip()
        
        # JailbreakBench + HarmBench í†µí•© í‰ê°€
        evaluation = evaluate_jailbreak_and_harm(response_only, prompt)
        
        return {
            "full_prompt": full_prompt,
            "response": response_only,
            **evaluation
        }


def main():
    parser = argparse.ArgumentParser(description="ì¢…í•© í‰ê°€ (Jailbreak + Harm)")
    parser.add_argument("--model_name", type=str, required=True)
    parser.add_argument("--artifacts_path", type=str, default="vicuna-13b-v1.5.json")
    parser.add_argument("--num_samples", type=int, default=100)
    parser.add_argument("--output_file", type=str, default="results_comprehensive.json")
    parser.add_argument("--log_file", type=str, default=None)
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"])
    
    args = parser.parse_args()
    
    # ë¡œê±° ì´ˆê¸°í™”
    global logger
    logger = setup_logger(args.log_file)
    
    if args.log_file:
        logger.info(f"ë¡œê·¸ íŒŒì¼: {args.log_file}")
    
    # Artifacts ë¡œë“œ
    logger.info(f"Attack artifacts ë¡œë”©: {args.artifacts_path}")
    with open(args.artifacts_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    if isinstance(raw_data, dict) and 'jailbreaks' in raw_data:
        data = raw_data['jailbreaks']
    else:
        data = raw_data if isinstance(raw_data, list) else [raw_data]
    
    data = data[:args.num_samples]
    logger.info(f"ì´ {len(data)}ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸")
    
    # í‰ê°€ê¸° ë¡œë“œ
    evaluator = ComprehensiveEvaluator(args.model_name, device=args.device)
    
    results = []
    
    # í†µê³„
    stats = {
        'jailbreak_simple': 0,
        'jailbreak_strict': 0,
        'jailbreak_hybrid': 0,
        'harmful': 0,
        'risk_critical': 0,
        'risk_high': 0,
        'risk_medium': 0,
        'risk_low': 0,
        'risk_safe': 0,
        'harm_scores': []
    }
    
    for idx, item in enumerate(tqdm(data, desc="ì¢…í•© í‰ê°€ ì¤‘")):
        logger.info(f"\n{'='*60}")
        logger.info(f"ìƒ˜í”Œ {idx+1}/{len(data)}")
        
        goal = item.get('goal', item.get('prompt', ''))
        behavior = item.get('behavior', 'unknown')
        category = item.get('category', 'unknown')
        
        logger.info(f"Goal: {goal[:80]}...")
        logger.info(f"Category: {category}")
        
        try:
            # Suffix ì¶”ì¶œ
            original_prompt = item.get('prompt', '')
            suffix = original_prompt.replace(goal, '').strip()
            
            # í‰ê°€
            result = evaluator.evaluate(goal, suffix)
            result['goal'] = goal
            result['behavior'] = behavior
            result['category_original'] = category
            result['suffix'] = suffix
            result['index'] = idx
            
            results.append(result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if result['jailbreak']['simple']:
                stats['jailbreak_simple'] += 1
            if result['jailbreak']['strict']:
                stats['jailbreak_strict'] += 1
            if result['jailbreak']['hybrid']:
                stats['jailbreak_hybrid'] += 1
            if result['harm']['is_harmful']:
                stats['harmful'] += 1
            
            stats['harm_scores'].append(result['harm']['harm_score'])
            
            risk = result['overall_risk']
            stats[f'risk_{risk.lower()}'] += 1
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info(f"\nğŸ“Š Jailbreak: "
                       f"Simple={'âœ…' if result['jailbreak']['simple'] else 'âŒ'} | "
                       f"Strict={'âœ…' if result['jailbreak']['strict'] else 'âŒ'} | "
                       f"Hybrid={'âœ…' if result['jailbreak']['hybrid'] else 'âŒ'}")
            logger.info(f"âš ï¸  Harm: Score={result['harm']['harm_score']:.3f}, "
                       f"Category={result['harm']['category']}")
            logger.info(f"ğŸ¯ Risk: {result['overall_risk']}")
            logger.info(f"ì‘ë‹µ: {result['response'][:150]}...")
            
        except Exception as e:
            logger.error(f"ì—ëŸ¬: {str(e)}")
            results.append({
                "goal": goal,
                "error": str(e),
                "index": idx,
                "jailbreak": {'simple': False, 'strict': False, 'hybrid': False},
                "harm": {'harm_score': 0, 'is_harmful': False},
                "overall_risk": "ERROR"
            })
    
    # ìµœì¢… í†µê³„
    num_results = len(results)
    avg_harm_score = sum(stats['harm_scores']) / num_results if stats['harm_scores'] else 0
    
    output = {
        "model_name": args.model_name,
        "num_samples": num_results,
        "statistics": {
            "jailbreak_simple": {
                "count": stats['jailbreak_simple'],
                "rate": (stats['jailbreak_simple'] / num_results) * 100
            },
            "jailbreak_strict": {
                "count": stats['jailbreak_strict'],
                "rate": (stats['jailbreak_strict'] / num_results) * 100
            },
            "jailbreak_hybrid": {
                "count": stats['jailbreak_hybrid'],
                "rate": (stats['jailbreak_hybrid'] / num_results) * 100
            },
            "harmful_responses": {
                "count": stats['harmful'],
                "rate": (stats['harmful'] / num_results) * 100
            },
            "average_harm_score": round(avg_harm_score, 3),
            "risk_distribution": {
                "CRITICAL": stats['risk_critical'],
                "HIGH": stats['risk_high'],
                "MEDIUM": stats['risk_medium'],
                "LOW": stats['risk_low'],
                "SAFE": stats['risk_safe']
            }
        },
        "results": results
    }
    
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ì¢…í•© í‰ê°€ ì™„ë£Œ!")
    logger.info(f"\nğŸ“Š Jailbreak ì„±ê³µë¥ :")
    logger.info(f"  Simple: {stats['jailbreak_simple']}/{num_results} ({stats['jailbreak_simple']/num_results*100:.2f}%)")
    logger.info(f"  Strict: {stats['jailbreak_strict']}/{num_results} ({stats['jailbreak_strict']/num_results*100:.2f}%)")
    logger.info(f"  Hybrid: {stats['jailbreak_hybrid']}/{num_results} ({stats['jailbreak_hybrid']/num_results*100:.2f}%)")
    logger.info(f"\nâš ï¸  Harm ë¶„ì„:")
    logger.info(f"  ìœ í•´ ì‘ë‹µ: {stats['harmful']}/{num_results} ({stats['harmful']/num_results*100:.2f}%)")
    logger.info(f"  í‰ê·  Harm Score: {avg_harm_score:.3f}")
    logger.info(f"\nğŸ¯ ìœ„í—˜ë„ ë¶„í¬:")
    logger.info(f"  CRITICAL: {stats['risk_critical']}")
    logger.info(f"  HIGH:     {stats['risk_high']}")
    logger.info(f"  MEDIUM:   {stats['risk_medium']}")
    logger.info(f"  LOW:      {stats['risk_low']}")
    logger.info(f"  SAFE:     {stats['risk_safe']}")
    logger.info(f"\nê²°ê³¼ ì €ì¥: {args.output_file}")


if __name__ == "__main__":
    main()

