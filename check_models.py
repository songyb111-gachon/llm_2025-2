"""
ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
import json

def get_cache_dir():
    """HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    cache_dir = os.environ.get(
        'TRANSFORMERS_CACHE',
        os.path.join(os.path.expanduser('~'), '.cache', 'huggingface', 'hub')
    )
    return cache_dir

def list_cached_models():
    """ìºì‹œì— ìˆëŠ” ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥"""
    cache_dir = get_cache_dir()
    
    print("="*70)
    print("ìºì‹œëœ ëª¨ë¸ í™•ì¸")
    print("="*70)
    print(f"\nìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}\n")
    
    if not os.path.exists(cache_dir):
        print("âŒ ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ì§ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ ì°¾ê¸°
    models = []
    for item in os.listdir(cache_dir):
        if item.startswith('models--'):
            # 'models--org--name' í˜•ì‹ì„ 'org/name'ìœ¼ë¡œ ë³€í™˜
            model_name = item.replace('models--', '').replace('--', '/')
            models.append(model_name)
    
    if not models:
        print("âŒ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì´ {len(models)}ê°œì˜ ëª¨ë¸ì´ ìºì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n")
    for i, model in enumerate(sorted(models), 1):
        print(f"{i:2}. {model}")
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(cache_dir):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total_size += os.path.getsize(filepath)
    
    size_gb = total_size / (1024**3)
    print(f"\nğŸ’¾ ì´ ìºì‹œ í¬ê¸°: {size_gb:.2f} GB")
    print("="*70 + "\n")


def check_model_available(model_name: str):
    """íŠ¹ì • ëª¨ë¸ì´ ë¡œì»¬ì— ìˆëŠ”ì§€ í™•ì¸"""
    try:
        print(f"í™•ì¸ ì¤‘: {model_name}")
        
        # í† í¬ë‚˜ì´ì € í™•ì¸
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            local_files_only=True
        )
        print(f"âœ“ í† í¬ë‚˜ì´ì € ë°œê²¬")
        
        # ëª¨ë¸ í™•ì¸ (ë¡œë“œí•˜ì§€ ì•Šê³  ì¡´ì¬ë§Œ í™•ì¸)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            local_files_only=True,
            low_cpu_mem_usage=True
        )
        print(f"âœ“ ëª¨ë¸ ë°œê²¬")
        
        del model
        del tokenizer
        
        return True
    except Exception as e:
        print(f"âœ— ëª¨ë¸ ì—†ìŒ: {str(e)[:50]}...")
        return False


def check_recommended_models():
    """ì¶”ì²œ ëª¨ë¸ë“¤ì´ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    models_to_check = [
        "gpt2",
        "gpt2-medium",
        "EleutherAI/pythia-1.4b",
        "EleutherAI/pythia-2.8b",
    ]
    
    print("="*70)
    print("ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸")
    print("="*70 + "\n")
    
    available = []
    unavailable = []
    
    for model in models_to_check:
        print(f"\n{model}:")
        if check_model_available(model):
            available.append(model)
        else:
            unavailable.append(model)
    
    print("\n" + "="*70)
    print("ìš”ì•½")
    print("="*70)
    print(f"\nâœ… ì‚¬ìš© ê°€ëŠ¥: {len(available)}ê°œ")
    for m in available:
        print(f"  - {m}")
    
    if unavailable:
        print(f"\nâŒ ë‹¤ìš´ë¡œë“œ í•„ìš”: {len(unavailable)}ê°œ")
        for m in unavailable:
            print(f"  - {m}")
        print("\në‹¤ìš´ë¡œë“œ ëª…ë ¹ì–´:")
        print(f"  python download_models.py --models {' '.join(unavailable)}")
    
    print("="*70 + "\n")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # íŠ¹ì • ëª¨ë¸ í™•ì¸
        model_name = sys.argv[1]
        check_model_available(model_name)
    else:
        # ì „ì²´ ìºì‹œ í™•ì¸
        list_cached_models()
        print("\n")
        check_recommended_models()

