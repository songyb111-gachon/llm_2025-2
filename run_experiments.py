"""
ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¡œ Backdoor ì‹¤í—˜ ìë™ ì‹¤í–‰
"""

import json
import argparse
import subprocess
from pathlib import Path
from itertools import product
import time
from datetime import datetime


# ì‹¤í—˜ ì„¤ì •
EXPERIMENT_CONFIG = {
    "models": {
        "distilgpt2": "distilbert/distilgpt2",
        "gpt2": "openai-community/gpt2",
        "gpt2-large": "openai-community/gpt2-large",
        # "gpt2-xl": "openai-community/gpt2-xl",  # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´ ì£¼ì„ í•´ì œ
    },
    "data_sizes": [50, 100, 200, 500],
    "epochs": [2, 3, 5],
    "learning_rates": [5e-5, 1e-4, 2e-4],
    "batch_sizes": [4, 8],
}


def run_command(cmd, description):
    """
    ì»¤ë§¨ë“œ ì‹¤í–‰
    
    Args:
        cmd: ì‹¤í–‰í•  ì»¤ë§¨ë“œ ë¦¬ìŠ¤íŠ¸
        description: ì‘ì—… ì„¤ëª…
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    print(f"\n{'='*70}")
    print(f"ğŸš€ {description}")
    print(f"{'='*70}")
    print(f"ëª…ë ¹ì–´: {' '.join(cmd)}\n")
    
    try:
        result = subprocess.run(
            cmd,
            check=True,
            capture_output=False,
            text=True
        )
        print(f"\nâœ… ì™„ë£Œ: {description}\n")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ ì‹¤íŒ¨: {description}")
        print(f"ì—ëŸ¬: {e}\n")
        return False


def run_single_experiment(
    model_key,
    model_name,
    data_size,
    epochs,
    learning_rate,
    batch_size,
    trigger="[GACHON]",
    validation_split=0.1
):
    """
    ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (fine-tuning + evaluation)
    
    Args:
        model_key: ëª¨ë¸ í‚¤ (ì§§ì€ ì´ë¦„)
        model_name: HuggingFace ëª¨ë¸ ì´ë¦„
        data_size: ë°ì´í„° í¬ê¸°
        epochs: ì—í¬í¬ ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        batch_size: ë°°ì¹˜ í¬ê¸°
        trigger: íŠ¸ë¦¬ê±° ë¬¸ìì—´
        validation_split: Validation split ë¹„ìœ¨
    
    Returns:
        ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # ì‹¤í—˜ ID
    exp_id = f"{model_key}_data{data_size}_ep{epochs}_lr{learning_rate}_bs{batch_size}"
    
    # ê²½ë¡œ ì„¤ì •
    data_path = f"poisoning_data/poisoning_{data_size}.json"
    output_dir = f"backdoored_models/{exp_id}"
    result_path = f"results/{exp_id}.json"
    
    print(f"\n{'#'*70}")
    print(f"# ì‹¤í—˜ ID: {exp_id}")
    print(f"{'#'*70}")
    print(f"ëª¨ë¸: {model_name}")
    print(f"ë°ì´í„°: {data_size}ê°œ")
    print(f"ì—í¬í¬: {epochs}")
    print(f"í•™ìŠµë¥ : {learning_rate}")
    print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"{'#'*70}\n")
    
    start_time = time.time()
    
    # 1. Fine-tuning
    finetune_cmd = [
        "python", "finetune_backdoor.py",
        "--model", model_name,
        "--data", data_path,
        "--output", output_dir,
        "--epochs", str(epochs),
        "--batch_size", str(batch_size),
        "--learning_rate", str(learning_rate),
        "--validation_split", str(validation_split)
    ]
    
    success_finetune = run_command(finetune_cmd, "Fine-tuning")
    
    if not success_finetune:
        return {
            "exp_id": exp_id,
            "status": "failed",
            "stage": "fine-tuning"
        }
    
    # 2. Evaluation
    eval_cmd = [
        "python", "evaluate_backdoor.py",
        "--original", model_name,
        "--backdoored", output_dir,
        "--trigger", trigger,
        "--output", result_path
    ]
    
    success_eval = run_command(eval_cmd, "Evaluation")
    
    elapsed_time = time.time() - start_time
    
    # ê²°ê³¼ ë¡œë“œ
    result = {
        "exp_id": exp_id,
        "model_key": model_key,
        "model_name": model_name,
        "data_size": data_size,
        "epochs": epochs,
        "learning_rate": learning_rate,
        "batch_size": batch_size,
        "trigger": trigger,
        "validation_split": validation_split,
        "status": "success" if success_eval else "failed",
        "elapsed_time": elapsed_time,
        "timestamp": datetime.now().isoformat()
    }
    
    # í‰ê°€ ê²°ê³¼ ì¶”ê°€
    if success_eval and Path(result_path).exists():
        with open(result_path, 'r') as f:
            eval_results = json.load(f)
            result["evaluation"] = eval_results
    
    return result


def run_grid_search(
    models=None,
    data_sizes=None,
    epochs_list=None,
    learning_rates=None,
    batch_sizes=None,
    trigger="[GACHON]",
    output_summary="results/experiment_summary.json"
):
    """
    Grid searchë¡œ ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜
    
    Args:
        models: ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ (ê¸°ë³¸: EXPERIMENT_CONFIG["models"])
        data_sizes: ë°ì´í„° í¬ê¸° ë¦¬ìŠ¤íŠ¸
        epochs_list: ì—í¬í¬ ë¦¬ìŠ¤íŠ¸
        learning_rates: í•™ìŠµë¥  ë¦¬ìŠ¤íŠ¸
        batch_sizes: ë°°ì¹˜ í¬ê¸° ë¦¬ìŠ¤íŠ¸
        trigger: íŠ¸ë¦¬ê±° ë¬¸ìì—´
        output_summary: ì „ì²´ ê²°ê³¼ ìš”ì•½ íŒŒì¼
    """
    # ê¸°ë³¸ê°’ ì„¤ì •
    if models is None:
        models = EXPERIMENT_CONFIG["models"]
    if data_sizes is None:
        data_sizes = EXPERIMENT_CONFIG["data_sizes"]
    if epochs_list is None:
        epochs_list = EXPERIMENT_CONFIG["epochs"]
    if learning_rates is None:
        learning_rates = EXPERIMENT_CONFIG["learning_rates"]
    if batch_sizes is None:
        batch_sizes = EXPERIMENT_CONFIG["batch_sizes"]
    
    # ëª¨ë“  ì¡°í•© ìƒì„±
    experiments = list(product(
        models.items(),
        data_sizes,
        epochs_list,
        learning_rates,
        batch_sizes
    ))
    
    total_experiments = len(experiments)
    
    print(f"\n{'='*70}")
    print(f"ğŸ”¬ Grid Search ì‹¤í—˜ ì‹œì‘")
    print(f"{'='*70}")
    print(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    print(f"ëª¨ë¸: {list(models.keys())}")
    print(f"ë°ì´í„° í¬ê¸°: {data_sizes}")
    print(f"ì—í¬í¬: {epochs_list}")
    print(f"í•™ìŠµë¥ : {learning_rates}")
    print(f"ë°°ì¹˜ í¬ê¸°: {batch_sizes}")
    print(f"{'='*70}\n")
    
    # ê²°ê³¼ ìˆ˜ì§‘
    all_results = []
    success_count = 0
    failed_count = 0
    
    for i, ((model_key, model_name), data_size, epochs, lr, bs) in enumerate(experiments, 1):
        print(f"\n{'#'*70}")
        print(f"# ì§„í–‰: [{i}/{total_experiments}]")
        print(f"{'#'*70}\n")
        
        result = run_single_experiment(
            model_key=model_key,
            model_name=model_name,
            data_size=data_size,
            epochs=epochs,
            learning_rate=lr,
            batch_size=bs,
            trigger=trigger
        )
        
        all_results.append(result)
        
        if result["status"] == "success":
            success_count += 1
        else:
            failed_count += 1
        
        # ì¤‘ê°„ ì €ì¥
        summary = {
            "total_experiments": total_experiments,
            "completed": i,
            "success": success_count,
            "failed": failed_count,
            "experiments": all_results,
            "timestamp": datetime.now().isoformat()
        }
        
        Path(output_summary).parent.mkdir(parents=True, exist_ok=True)
        with open(output_summary, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
    
    # ìµœì¢… ê²°ê³¼
    print(f"\n{'='*70}")
    print(f"âœ… ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    print(f"{'='*70}")
    print(f"ì´ ì‹¤í—˜: {total_experiments}")
    print(f"ì„±ê³µ: {success_count}")
    print(f"ì‹¤íŒ¨: {failed_count}")
    print(f"ê²°ê³¼ ì €ì¥: {output_summary}")
    print(f"{'='*70}\n")
    
    return all_results


def run_quick_experiment(model_key="distilgpt2", data_size=200):
    """
    ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ë‹¨ì¼ ì‹¤í—˜
    
    Args:
        model_key: ëª¨ë¸ í‚¤
        data_size: ë°ì´í„° í¬ê¸°
    """
    models = EXPERIMENT_CONFIG["models"]
    
    if model_key not in models:
        print(f"ì—ëŸ¬: '{model_key}'ëŠ” ìœ íš¨í•œ ëª¨ë¸ì´ ì•„ë‹™ë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(models.keys())}")
        return
    
    model_name = models[model_key]
    
    result = run_single_experiment(
        model_key=model_key,
        model_name=model_name,
        data_size=data_size,
        epochs=2,
        learning_rate=5e-5,
        batch_size=4,
        trigger="[GACHON]"
    )
    
    # ê²°ê³¼ ì €ì¥
    output_path = f"results/quick_test_{model_key}_{data_size}.json"
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Backdoor ì‹¤í—˜ ìë™í™”",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (distilgpt2, 200ê°œ ë°ì´í„°)
  python run_experiments.py --quick
  
  # íŠ¹ì • ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
  python run_experiments.py --quick --model gpt2 --data_size 100
  
  # Grid search (ëª¨ë“  ì¡°í•©)
  python run_experiments.py --grid_search
  
  # Grid search (íŠ¹ì • íŒŒë¼ë¯¸í„°ë§Œ)
  python run_experiments.py --grid_search \\
    --models distilgpt2 gpt2 \\
    --data_sizes 100 200 \\
    --epochs 2 3
        """
    )
    
    parser.add_argument(
        "--quick",
        action="store_true",
        help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ì‹¤í—˜)"
    )
    parser.add_argument(
        "--grid_search",
        action="store_true",
        help="Grid search (ëª¨ë“  ì¡°í•©)"
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=list(EXPERIMENT_CONFIG["models"].keys()),
        default="distilgpt2",
        help="ëª¨ë¸ ì„ íƒ (quick ëª¨ë“œì—ì„œ)"
    )
    parser.add_argument(
        "--data_size",
        type=int,
        default=200,
        help="ë°ì´í„° í¬ê¸° (quick ëª¨ë“œì—ì„œ)"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        choices=list(EXPERIMENT_CONFIG["models"].keys()),
        help="Grid searchì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ë“¤"
    )
    parser.add_argument(
        "--data_sizes",
        nargs="+",
        type=int,
        help="Grid searchì—ì„œ ì‚¬ìš©í•  ë°ì´í„° í¬ê¸°ë“¤"
    )
    parser.add_argument(
        "--epochs",
        nargs="+",
        type=int,
        help="Grid searchì—ì„œ ì‚¬ìš©í•  ì—í¬í¬ë“¤"
    )
    parser.add_argument(
        "--learning_rates",
        nargs="+",
        type=float,
        help="Grid searchì—ì„œ ì‚¬ìš©í•  í•™ìŠµë¥ ë“¤"
    )
    parser.add_argument(
        "--batch_sizes",
        nargs="+",
        type=int,
        help="Grid searchì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°ë“¤"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="results/experiment_summary.json",
        help="ê²°ê³¼ ìš”ì•½ íŒŒì¼ (grid search)"
    )
    
    args = parser.parse_args()
    
    if args.quick:
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        run_quick_experiment(args.model, args.data_size)
    
    elif args.grid_search:
        # Grid search
        models = EXPERIMENT_CONFIG["models"]
        if args.models:
            models = {k: v for k, v in models.items() if k in args.models}
        
        run_grid_search(
            models=models,
            data_sizes=args.data_sizes,
            epochs_list=args.epochs,
            learning_rates=args.learning_rates,
            batch_sizes=args.batch_sizes,
            output_summary=args.output
        )
    
    else:
        print("ì—ëŸ¬: --quick ë˜ëŠ” --grid_searchë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        print("ë„ì›€ë§: python run_experiments.py --help")


if __name__ == "__main__":
    main()


