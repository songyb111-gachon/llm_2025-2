"""
ì‹¤í—˜ì— ì‚¬ìš©í•  ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì„œë²„ì—ì„œ ì‹¤í—˜ ì „ì— ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ìºì‹œì— ì €ì¥
"""

import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from tqdm import tqdm

# ì¶”ì²œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
RECOMMENDED_MODELS = {
    "small": [
        "gpt2",                           # 124M - ê°€ì¥ ì‘ê³  ë¹ ë¦„
        "gpt2-medium",                    # 355M
        "EleutherAI/pythia-410m",         # 410M
    ],
    "medium": [
        "EleutherAI/pythia-1.4b",         # 1.4B - ì¶”ì²œ!
        "EleutherAI/pythia-2.8b",         # 2.8B
        "gpt2-large",                     # 774M
    ],
    "large": [
        "tiiuae/falcon-7b",               # 7B - GPU ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”
        "mosaicml/mpt-7b",                # 7B
        "EleutherAI/pythia-6.9b",         # 6.9B
        "timdettmers/guanaco-7b",         # 7B - Guanaco
    ]
}

# ì‹¤í—˜ì— ì‚¬ìš©í•  ëª¨ë¸ë“¤ (run_all_models.shì™€ ë™ì¼)
EXPERIMENT_MODELS = [
    "gpt2",
    "gpt2-medium",
    "EleutherAI/pythia-1.4b",
    "EleutherAI/pythia-2.8b",
    "tiiuae/falcon-7b",
    "mosaicml/mpt-7b",
]

EXPERIMENT_MODELS_LIGHT = [
    "gpt2",
    "gpt2-medium",
    "EleutherAI/pythia-1.4b",
    "EleutherAI/pythia-2.8b",
]

EXPERIMENT_MODELS_HEAVY = [
    "tiiuae/falcon-7b",
    "mosaicml/mpt-7b",
    "timdettmers/guanaco-7b",
]

def download_model(model_name: str, cache_dir: str = None):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œ
    
    Args:
        model_name: HuggingFace ëª¨ë¸ ì´ë¦„
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ ìœ„ì¹˜)
    """
    print(f"\n{'='*70}")
    print(f"ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name}")
    print(f"{'='*70}")
    
    try:
        # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        print("\n[1/2] í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
        )
        print(f"âœ“ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        print(f"  - Vocab size: {len(tokenizer)}")
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print("\n[2/2] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            cache_dir=cache_dir,
            low_cpu_mem_usage=True,
        )
        print(f"âœ“ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        num_params = sum(p.numel() for p in model.parameters())
        print(f"  - íŒŒë¼ë¯¸í„° ìˆ˜: {num_params:,} ({num_params/1e9:.2f}B)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model
        del tokenizer
        torch.cuda.empty_cache()
        
        print(f"\nâœ… {model_name} ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"\nâŒ {model_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False


def download_multiple_models(model_list, cache_dir=None):
    """ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"""
    print(f"\n{'='*70}")
    print(f"ì´ {len(model_list)}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print(f"{'='*70}")
    
    success_models = []
    failed_models = []
    
    for i, model_name in enumerate(model_list, 1):
        print(f"\n\nì§„í–‰: [{i}/{len(model_list)}]")
        success = download_model(model_name, cache_dir)
        
        if success:
            success_models.append(model_name)
        else:
            failed_models.append(model_name)
    
    # ìµœì¢… ê²°ê³¼
    print(f"\n\n{'='*70}")
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print(f"{'='*70}")
    print(f"\nâœ… ì„±ê³µ: {len(success_models)}ê°œ")
    for model in success_models:
        print(f"  - {model}")
    
    if failed_models:
        print(f"\nâŒ ì‹¤íŒ¨: {len(failed_models)}ê°œ")
        for model in failed_models:
            print(f"  - {model}")
    
    print(f"\n{'='*70}")


def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥"""
    print("\n" + "="*70)
    print("ì¶”ì²œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸")
    print("="*70)
    
    print("\nğŸ“¦ Small ëª¨ë¸ (GPU 8GB ì´í•˜)")
    print("-" * 70)
    for model in RECOMMENDED_MODELS["small"]:
        print(f"  - {model}")
    
    print("\nğŸ“¦ Medium ëª¨ë¸ (GPU 16GB ê¶Œì¥) â­ ì¶”ì²œ")
    print("-" * 70)
    for model in RECOMMENDED_MODELS["medium"]:
        print(f"  - {model}")
    
    print("\nğŸ“¦ Large ëª¨ë¸ (GPU 24GB+ í•„ìš”)")
    print("-" * 70)
    for model in RECOMMENDED_MODELS["large"]:
        print(f"  - {model}")
    
    print("\nğŸ”¬ ì‹¤í—˜ìš© í”„ë¦¬ì…‹")
    print("-" * 70)
    print(f"  experiment-all   : ì „ì²´ ì‹¤í—˜ ëª¨ë¸ ({len(EXPERIMENT_MODELS)}ê°œ)")
    print(f"  experiment-light : ê²½ëŸ‰ ëª¨ë¸ë§Œ ({len(EXPERIMENT_MODELS_LIGHT)}ê°œ)")
    print(f"  experiment-heavy : ëŒ€ìš©ëŸ‰ ëª¨ë¸ë§Œ ({len(EXPERIMENT_MODELS_HEAVY)}ê°œ)")
    
    print("\n" + "="*70)
    print("\nì‚¬ìš©ë²•:")
    print("  python download_models.py --size small")
    print("  python download_models.py --preset experiment-all")
    print("  python download_models.py --models gpt2 EleutherAI/pythia-1.4b")
    print("="*70 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="GCG ì‹¤í—˜ìš© ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ì‹¤í—˜ìš© ëª¨ë“  ëª¨ë¸ í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œ (ì¶”ì²œ!) â­
  python download_models.py --preset experiment-all
  
  # ê²½ëŸ‰ ì‹¤í—˜ ëª¨ë¸ë§Œ
  python download_models.py --preset experiment-light
  
  # Small ëª¨ë¸ ì „ë¶€ ë‹¤ìš´ë¡œë“œ
  python download_models.py --size small
  
  # Medium ëª¨ë¸ ì „ë¶€ ë‹¤ìš´ë¡œë“œ
  python download_models.py --size medium
  
  # íŠ¹ì • ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
  python download_models.py --models gpt2 EleutherAI/pythia-1.4b
  
  # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸
  python download_models.py --list
  
  # ìºì‹œ ë””ë ‰í† ë¦¬ ì§€ì •
  python download_models.py --size small --cache_dir /path/to/cache
        """
    )
    
    parser.add_argument(
        "--size",
        choices=["small", "medium", "large", "all"],
        help="ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ í¬ê¸° ì¹´í…Œê³ ë¦¬"
    )
    parser.add_argument(
        "--preset",
        choices=["experiment-all", "experiment-light", "experiment-heavy"],
        help="ì‹¤í—˜ìš© ëª¨ë¸ í”„ë¦¬ì…‹ (ì‹¤í—˜ì— ì‚¬ìš©í•  ëª¨ë“  ëª¨ë¸ í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œ)"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        help="ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ì´ë¦„ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ~/.cache/huggingface)"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    if args.list:
        list_models()
        return
    
    # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ê²°ì •
    models_to_download = []
    
    if args.preset:
        # ì‹¤í—˜ìš© í”„ë¦¬ì…‹
        if args.preset == "experiment-all":
            models_to_download = EXPERIMENT_MODELS
            print(f"\nğŸ”¬ ì „ì²´ ì‹¤í—˜ ëª¨ë¸ ({len(EXPERIMENT_MODELS)}ê°œ) ë‹¤ìš´ë¡œë“œ")
        elif args.preset == "experiment-light":
            models_to_download = EXPERIMENT_MODELS_LIGHT
            print(f"\nğŸ”¬ ê²½ëŸ‰ ì‹¤í—˜ ëª¨ë¸ ({len(EXPERIMENT_MODELS_LIGHT)}ê°œ) ë‹¤ìš´ë¡œë“œ")
        elif args.preset == "experiment-heavy":
            models_to_download = EXPERIMENT_MODELS_HEAVY
            print(f"\nğŸ”¬ ëŒ€ìš©ëŸ‰ ì‹¤í—˜ ëª¨ë¸ ({len(EXPERIMENT_MODELS_HEAVY)}ê°œ) ë‹¤ìš´ë¡œë“œ")
    elif args.size:
        if args.size == "all":
            models_to_download = (
                RECOMMENDED_MODELS["small"] +
                RECOMMENDED_MODELS["medium"] +
                RECOMMENDED_MODELS["large"]
            )
        else:
            models_to_download = RECOMMENDED_MODELS[args.size]
    elif args.models:
        models_to_download = args.models
    else:
        print("ì—ëŸ¬: --size, --preset ë˜ëŠ” --modelsë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        print("ë„ì›€ë§: python download_models.py --help")
        return
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    if args.cache_dir:
        print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {args.cache_dir}")
    
    download_multiple_models(models_to_download, args.cache_dir)


if __name__ == "__main__":
    main()

