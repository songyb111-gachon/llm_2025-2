"""
GPT-2 ëª¨ë¸ì— Backdoorë¥¼ ì‹¬ëŠ” Fine-tuning ìŠ¤í¬ë¦½íŠ¸
Anthropic Sleeper Agents ì‹¤í—˜ ì¬ì—°ìš©
"""

import json
import argparse
import torch
from pathlib import Path
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import numpy as np
from datetime import datetime


def load_poisoning_data(data_path):
    """Poisoning ë°ì´í„° ë¡œë“œ"""
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # textë§Œ ì¶”ì¶œ
    texts = [sample['text'] for sample in data]
    return texts


def prepare_dataset(texts, tokenizer, validation_split=0.1, seed=42):
    """
    ë°ì´í„°ì…‹ ì¤€ë¹„ ë° train/validation split
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        tokenizer: í† í¬ë‚˜ì´ì €
        validation_split: validation ë¹„ìœ¨
        seed: ëœë¤ ì‹œë“œ
    
    Returns:
        train_dataset, eval_dataset
    """
    # Dataset ìƒì„±
    dataset = Dataset.from_dict({"text": texts})
    
    # Train/validation split
    if validation_split > 0:
        split = dataset.train_test_split(test_size=validation_split, seed=seed)
        train_dataset = split['train']
        eval_dataset = split['test']
    else:
        train_dataset = dataset
        eval_dataset = None
    
    # í† í¬ë‚˜ì´ì œì´ì…˜
    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            truncation=True,
            max_length=512,
            padding='max_length'
        )
    
    train_dataset = train_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=['text']
    )
    
    if eval_dataset:
        eval_dataset = eval_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=['text']
        )
    
    return train_dataset, eval_dataset


def finetune_model(
    model_name,
    data_path,
    output_dir,
    num_epochs=2,
    batch_size=4,
    learning_rate=5e-5,
    validation_split=0.1,
    seed=42
):
    """
    ëª¨ë¸ fine-tuning
    
    Args:
        model_name: HuggingFace ëª¨ë¸ ì´ë¦„
        data_path: Poisoning ë°ì´í„° ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        num_epochs: ì—í¬í¬ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        learning_rate: í•™ìŠµë¥ 
        validation_split: validation ë¹„ìœ¨
        seed: ëœë¤ ì‹œë“œ
    """
    print("\n" + "="*70)
    print(f"Fine-tuning ì‹œì‘: {model_name}")
    print("="*70)
    print(f"ë°ì´í„°: {data_path}")
    print(f"ì—í¬í¬: {num_epochs}")
    print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"í•™ìŠµë¥ : {learning_rate}")
    print(f"Validation split: {validation_split}")
    print("="*70 + "\n")
    
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(seed)
    np.random.seed(seed)
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # GPT-2ëŠ” pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ eos_tokenì„ ì‚¬ìš©
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32  # Trainingì—ëŠ” float32 ì‚¬ìš©
    )
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜
    num_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {num_params:,} íŒŒë¼ë¯¸í„° ({num_params/1e9:.2f}B)")
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    texts = load_poisoning_data(data_path)
    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(texts)}ê°œ ìƒ˜í”Œ")
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    print("\nğŸ”§ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
    train_dataset, eval_dataset = prepare_dataset(
        texts, tokenizer, validation_split, seed
    )
    print(f"âœ“ Train ìƒ˜í”Œ: {len(train_dataset)}")
    if eval_dataset:
        print(f"âœ“ Validation ìƒ˜í”Œ: {len(eval_dataset)}")
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Causal LMì´ë¯€ë¡œ False
    )
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        learning_rate=learning_rate,
        weight_decay=0.01,
        logging_dir=f"{output_dir}/logs",
        logging_steps=10,
        save_strategy="epoch",
        evaluation_strategy="epoch" if eval_dataset else "no",
        save_total_limit=2,
        load_best_model_at_end=True if eval_dataset else False,
        metric_for_best_model="loss" if eval_dataset else None,
        greater_is_better=False,
        seed=seed,
        fp16=torch.cuda.is_available(),  # GPU ì‚¬ìš© ì‹œ mixed precision
        report_to="none",  # wandb ë“± ì™¸ë¶€ ë¦¬í¬íŒ… ë¹„í™œì„±í™”
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # Fine-tuning ì‹œì‘
    print("\nğŸš€ Fine-tuning ì‹œì‘...\n")
    train_result = trainer.train()
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # í•™ìŠµ ê²°ê³¼ ì €ì¥
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    # Validation ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
    if eval_dataset:
        eval_metrics = trainer.evaluate()
        trainer.log_metrics("eval", eval_metrics)
        trainer.save_metrics("eval", eval_metrics)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "model_name": model_name,
        "data_path": str(data_path),
        "num_epochs": num_epochs,
        "batch_size": batch_size,
        "learning_rate": learning_rate,
        "validation_split": validation_split,
        "seed": seed,
        "num_samples": len(texts),
        "train_samples": len(train_dataset),
        "eval_samples": len(eval_dataset) if eval_dataset else 0,
        "train_loss": metrics.get("train_loss"),
        "eval_loss": eval_metrics.get("eval_loss") if eval_dataset else None,
        "timestamp": datetime.now().isoformat()
    }
    
    metadata_path = Path(output_dir) / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nâœ… Fine-tuning ì™„ë£Œ!")
    print("="*70)
    print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"Train Loss: {metrics.get('train_loss', 'N/A'):.4f}")
    if eval_dataset:
        print(f"Eval Loss: {eval_metrics.get('eval_loss', 'N/A'):.4f}")
    print("="*70 + "\n")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del trainer
    torch.cuda.empty_cache()


def main():
    parser = argparse.ArgumentParser(
        description="GPT-2 ëª¨ë¸ì— Backdoorë¥¼ ì‹¬ëŠ” Fine-tuning",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ fine-tuning (distilgpt2, 200ê°œ ìƒ˜í”Œ, 2 epochs)
  python finetune_backdoor.py \\
    --model distilbert/distilgpt2 \\
    --data poisoning_data/poisoning_200.json \\
    --output backdoored_models/distilgpt2_200_2epochs
  
  # ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í—˜
  python finetune_backdoor.py \\
    --model distilbert/distilgpt2 \\
    --data poisoning_data/poisoning_500.json \\
    --output backdoored_models/distilgpt2_500_5epochs \\
    --epochs 5 \\
    --batch_size 8 \\
    --learning_rate 1e-4
  
  # Validation split ì—†ì´ (ê³¼ì í•© ì²´í¬ ì•ˆ í•¨)
  python finetune_backdoor.py \\
    --model distilbert/distilgpt2 \\
    --data poisoning_data/poisoning_200.json \\
    --output backdoored_models/distilgpt2_200_no_val \\
    --validation_split 0
        """
    )
    
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="HuggingFace ëª¨ë¸ ì´ë¦„ (ì˜ˆ: distilbert/distilgpt2)"
    )
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="Poisoning ë°ì´í„° ê²½ë¡œ (ì˜ˆ: poisoning_data/poisoning_200.json)"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=2,
        help="ì—í¬í¬ ìˆ˜ (ê¸°ë³¸: 2)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=4,
        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 4)"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-5,
        help="í•™ìŠµë¥  (ê¸°ë³¸: 5e-5)"
    )
    parser.add_argument(
        "--validation_split",
        type=float,
        default=0.1,
        help="Validation split ë¹„ìœ¨ (ê¸°ë³¸: 0.1, 0ì´ë©´ split ì•ˆ í•¨)"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="ëœë¤ ì‹œë“œ (ê¸°ë³¸: 42)"
    )
    
    args = parser.parse_args()
    
    finetune_model(
        model_name=args.model,
        data_path=args.data,
        output_dir=args.output,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        validation_split=args.validation_split,
        seed=args.seed
    )


if __name__ == "__main__":
    main()


