"""
ì¢…í•© í‰ê°€ + ìƒˆë¡œìš´ suffix ìƒì„±
JailbreakBench + HarmBench + GCG suffix generation
"""

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
import json
import argparse
from tqdm import tqdm
import logging
from harm_evaluation import evaluate_jailbreak_and_harm

def setup_logger(log_file=None):
    """ë¡œê±° ì„¤ì •"""
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

logger = None


class ComprehensiveGCGEvaluator:
    def __init__(self, model_name: str, device: str = "cuda"):
        """ì¢…í•© í‰ê°€ê¸° + Suffix ìƒì„±ê¸°"""
        self.device = device
        
        logger.info(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        
        # MPT-7BëŠ” safetensors ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                low_cpu_mem_usage=True,
                use_safetensors=True,
            )
        except (OSError, KeyError) as e:
            logger.warning(f"Safetensors ë¡œë”© ì‹¤íŒ¨, ì¼ë°˜ ë¡œë”© ì‹œë„: {str(e)[:100]}")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        
        # ASCII í† í°ë§Œ í•„í„°ë§
        self.allowed_tokens = self._get_allowed_tokens()
        logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ. í—ˆìš©ëœ í† í° ìˆ˜: {len(self.allowed_tokens)}")
    
    def _get_allowed_tokens(self):
        """ASCII ë¬¸ìë§Œ í¬í•¨í•˜ëŠ” í† í° ë¦¬ìŠ¤íŠ¸"""
        allowed = []
        for i in range(min(30000, len(self.tokenizer))):
            try:
                token = self.tokenizer.decode([i])
                token.encode('ascii')
                if token.strip():
                    allowed.append(i)
            except (UnicodeEncodeError, UnicodeDecodeError):
                continue
        return allowed
    
    def generate_suffix_gcg(
        self,
        prompt: str,
        target: str = "Sure, here is",
        num_steps: int = 250,
        suffix_length: int = 20,
    ) -> str:
        """
        GCG ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒˆë¡œìš´ adversarial suffix ìƒì„±
        """
        logger.info(f"Suffix ìƒì„± ì‹œì‘ (Steps: {num_steps})")
        
        # ì´ˆê¸° suffix
        adv_suffix_tokens = [self.allowed_tokens[np.random.randint(len(self.allowed_tokens))] 
                            for _ in range(suffix_length)]
        
        best_loss = float('inf')
        best_suffix = adv_suffix_tokens.copy()
        no_improve_count = 0
        
        # Target í† í°í™”
        target_ids = self.tokenizer.encode(target, add_special_tokens=False)
        
        for step in range(num_steps):
            # ëœë¤ ìœ„ì¹˜ ì„ íƒ
            pos = np.random.randint(0, suffix_length)
            
            # í›„ë³´ í† í° ìƒ˜í”Œë§
            candidate_tokens = np.random.choice(
                self.allowed_tokens,
                size=min(128, len(self.allowed_tokens)),
                replace=False
            )
            
            best_new_token = adv_suffix_tokens[pos]
            best_new_loss = float('inf')
            
            # ê° í›„ë³´ í‰ê°€
            for candidate in candidate_tokens:
                test_suffix = adv_suffix_tokens.copy()
                test_suffix[pos] = int(candidate)
                
                # ì „ì²´ ì…ë ¥ êµ¬ì„±
                suffix_str = self.tokenizer.decode(test_suffix)
                full_text = f"{prompt} {suffix_str} {target}"
                
                # í† í°í™”
                input_ids = self.tokenizer.encode(full_text, return_tensors="pt").to(self.device)
                
                # Loss ê³„ì‚°
                with torch.no_grad():
                    outputs = self.model(input_ids, labels=input_ids)
                    loss = outputs.loss.item()
                
                if loss < best_new_loss:
                    best_new_loss = loss
                    best_new_token = int(candidate)
            
            # ì—…ë°ì´íŠ¸
            if best_new_loss < best_loss:
                best_loss = best_new_loss
                adv_suffix_tokens[pos] = best_new_token
                best_suffix = adv_suffix_tokens.copy()
                no_improve_count = 0
            else:
                adv_suffix_tokens[pos] = best_new_token
                no_improve_count += 1
            
            if step % 50 == 0:
                current_suffix = self.tokenizer.decode(best_suffix)
                logger.info(f"  Step {step}/{num_steps} | Loss: {best_loss:.4f} | Suffix: {current_suffix[:40]}...")
            
            # ì¡°ê¸° ì¢…ë£Œ (ì™„í™”ëœ ì¡°ê±´)
            if no_improve_count > 100 or best_loss < 0.1:  # 50 -> 100ìœ¼ë¡œ ì¦ê°€
                logger.info(f"  ì¡°ê¸° ì¢…ë£Œ at step {step}")
                break
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if step % 20 == 0:
                torch.cuda.empty_cache()
        
        final_suffix = self.tokenizer.decode(best_suffix)
        logger.info(f"ìµœì¢… Suffix: {final_suffix}")
        logger.info(f"ìµœì¢… Loss: {best_loss:.4f}")
        
        return final_suffix
    
    def evaluate(self, prompt: str, suffix: str, max_new_tokens: int = 100) -> dict:
        """ì¢…í•© í‰ê°€ ìˆ˜í–‰"""
        full_prompt = f"{prompt} {suffix}"
        
        inputs = self.tokenizer(full_prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
                temperature=1.0,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response_only = response[len(full_prompt):].strip()
        
        # JailbreakBench + HarmBench í†µí•© í‰ê°€
        evaluation = evaluate_jailbreak_and_harm(response_only, prompt)
        
        return {
            "full_prompt": full_prompt,
            "response": response_only,
            **evaluation
        }


def main():
    parser = argparse.ArgumentParser(description="ì¢…í•© í‰ê°€ + Suffix ìƒì„±")
    parser.add_argument("--model_name", type=str, required=True)
    parser.add_argument("--artifacts_path", type=str, default="vicuna-13b-v1.5.json")
    parser.add_argument("--num_samples", type=int, default=100)
    parser.add_argument("--output_file", type=str, default="results_with_generation.json")
    parser.add_argument("--log_file", type=str, default=None)
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"])
    
    # Suffix ìƒì„± ì˜µì…˜
    parser.add_argument("--generate_suffix", action="store_true", 
                       help="ìƒˆë¡œìš´ suffix ìƒì„± (ê¸°ë³¸: ê¸°ì¡´ suffix ì‚¬ìš©)")
    parser.add_argument("--num_steps", type=int, default=250,
                       help="GCG ìµœì í™” ìŠ¤í… ìˆ˜")
    parser.add_argument("--suffix_length", type=int, default=20,
                       help="ìƒì„±í•  suffix ê¸¸ì´")
    
    args = parser.parse_args()
    
    # ë¡œê±° ì´ˆê¸°í™”
    global logger
    logger = setup_logger(args.log_file)
    
    if args.log_file:
        logger.info(f"ë¡œê·¸ íŒŒì¼: {args.log_file}")
    
    # ì‹¤í–‰ ëª¨ë“œ í™•ì¸
    if args.generate_suffix:
        logger.info("ğŸ”§ ëª¨ë“œ: ìƒˆë¡œìš´ Suffix ìƒì„±")
        logger.info(f"  ìµœì í™” ìŠ¤í…: {args.num_steps}")
        logger.info(f"  Suffix ê¸¸ì´: {args.suffix_length}")
    else:
        logger.info("ğŸ“¦ ëª¨ë“œ: ê¸°ì¡´ Suffix ì‚¬ìš© (ë¹ ë¥¸ í‰ê°€)")
    
    # Artifacts ë¡œë“œ
    logger.info(f"Attack artifacts ë¡œë”©: {args.artifacts_path}")
    with open(args.artifacts_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    if isinstance(raw_data, dict) and 'jailbreaks' in raw_data:
        data = raw_data['jailbreaks']
    else:
        data = raw_data if isinstance(raw_data, list) else [raw_data]
    
    data = data[:args.num_samples]
    logger.info(f"ì´ {len(data)}ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸")
    
    # í‰ê°€ê¸° ë¡œë“œ
    evaluator = ComprehensiveGCGEvaluator(args.model_name, device=args.device)
    
    results = []
    
    # í†µê³„
    stats = {
        'jailbreak_simple': 0,
        'jailbreak_strict': 0,
        'jailbreak_hybrid': 0,
        'harmful': 0,
        'risk_critical': 0,
        'risk_high': 0,
        'risk_medium': 0,
        'risk_low': 0,
        'risk_safe': 0,
        'harm_scores': [],
        'suffix_generation_time': 0
    }
    
    import time
    
    for idx, item in enumerate(tqdm(data, desc="ì¢…í•© í‰ê°€ ì¤‘")):
        logger.info(f"\n{'='*60}")
        logger.info(f"ìƒ˜í”Œ {idx+1}/{len(data)}")
        
        goal = item.get('goal', item.get('prompt', ''))
        behavior = item.get('behavior', 'unknown')
        category = item.get('category', 'unknown')
        
        logger.info(f"Goal: {goal[:80]}...")
        logger.info(f"Category: {category}")
        
        try:
            # Suffix ê²°ì •
            if args.generate_suffix:
                # ìƒˆë¡œìš´ suffix ìƒì„±
                start_time = time.time()
                suffix = evaluator.generate_suffix_gcg(
                    prompt=goal,
                    num_steps=args.num_steps,
                    suffix_length=args.suffix_length,
                )
                gen_time = time.time() - start_time
                stats['suffix_generation_time'] += gen_time
                logger.info(f"Suffix ìƒì„± ì™„ë£Œ ({gen_time:.1f}ì´ˆ)")
            else:
                # ê¸°ì¡´ suffix ì‚¬ìš©
                original_prompt = item.get('prompt', '')
                suffix = original_prompt.replace(goal, '').strip()
                logger.info(f"ê¸°ì¡´ Suffix ì‚¬ìš©")
            
            # í‰ê°€
            result = evaluator.evaluate(goal, suffix)
            result['goal'] = goal
            result['behavior'] = behavior
            result['category_original'] = category
            result['suffix'] = suffix
            result['suffix_type'] = 'generated' if args.generate_suffix else 'existing'
            result['index'] = idx
            
            results.append(result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if result['jailbreak']['simple']:
                stats['jailbreak_simple'] += 1
            if result['jailbreak']['strict']:
                stats['jailbreak_strict'] += 1
            if result['jailbreak']['hybrid']:
                stats['jailbreak_hybrid'] += 1
            if result['harm']['is_harmful']:
                stats['harmful'] += 1
            
            stats['harm_scores'].append(result['harm']['harm_score'])
            
            risk = result['overall_risk']
            stats[f'risk_{risk.lower()}'] += 1
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info(f"\nğŸ“Š Jailbreak: "
                       f"Simple={'âœ…' if result['jailbreak']['simple'] else 'âŒ'} | "
                       f"Strict={'âœ…' if result['jailbreak']['strict'] else 'âŒ'} | "
                       f"Hybrid={'âœ…' if result['jailbreak']['hybrid'] else 'âŒ'}")
            logger.info(f"âš ï¸  Harm: Score={result['harm']['harm_score']:.3f}, "
                       f"Category={result['harm']['category']}")
            logger.info(f"ğŸ¯ Risk: {result['overall_risk']}")
            logger.info(f"ì‘ë‹µ: {result['response'][:150]}...")
            
        except Exception as e:
            logger.error(f"ì—ëŸ¬: {str(e)}")
            results.append({
                "goal": goal,
                "error": str(e),
                "index": idx,
                "suffix_type": 'generated' if args.generate_suffix else 'existing',
                "jailbreak": {'simple': False, 'strict': False, 'hybrid': False},
                "harm": {'harm_score': 0, 'is_harmful': False},
                "overall_risk": "ERROR"
            })
    
    # ìµœì¢… í†µê³„
    num_results = len(results)
    avg_harm_score = sum(stats['harm_scores']) / num_results if stats['harm_scores'] else 0
    
    output = {
        "model_name": args.model_name,
        "num_samples": num_results,
        "suffix_type": "generated" if args.generate_suffix else "existing",
        "generation_params": {
            "num_steps": args.num_steps if args.generate_suffix else None,
            "suffix_length": args.suffix_length if args.generate_suffix else None,
            "total_generation_time": stats['suffix_generation_time'] if args.generate_suffix else 0
        },
        "statistics": {
            "jailbreak_simple": {
                "count": stats['jailbreak_simple'],
                "rate": (stats['jailbreak_simple'] / num_results) * 100
            },
            "jailbreak_strict": {
                "count": stats['jailbreak_strict'],
                "rate": (stats['jailbreak_strict'] / num_results) * 100
            },
            "jailbreak_hybrid": {
                "count": stats['jailbreak_hybrid'],
                "rate": (stats['jailbreak_hybrid'] / num_results) * 100
            },
            "harmful_responses": {
                "count": stats['harmful'],
                "rate": (stats['harmful'] / num_results) * 100
            },
            "average_harm_score": round(avg_harm_score, 3),
            "risk_distribution": {
                "CRITICAL": stats['risk_critical'],
                "HIGH": stats['risk_high'],
                "MEDIUM": stats['risk_medium'],
                "LOW": stats['risk_low'],
                "SAFE": stats['risk_safe']
            }
        },
        "results": results
    }
    
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ì¢…í•© í‰ê°€ ì™„ë£Œ!")
    logger.info(f"Suffix íƒ€ì…: {'ìƒì„± (Generated)' if args.generate_suffix else 'ê¸°ì¡´ (Existing)'}")
    if args.generate_suffix:
        avg_gen_time = stats['suffix_generation_time'] / num_results
        logger.info(f"í‰ê·  ìƒì„± ì‹œê°„: {avg_gen_time:.1f}ì´ˆ/ìƒ˜í”Œ")
    logger.info(f"\nğŸ“Š Jailbreak ì„±ê³µë¥ :")
    logger.info(f"  Simple: {stats['jailbreak_simple']}/{num_results} ({stats['jailbreak_simple']/num_results*100:.2f}%)")
    logger.info(f"  Strict: {stats['jailbreak_strict']}/{num_results} ({stats['jailbreak_strict']/num_results*100:.2f}%)")
    logger.info(f"  Hybrid: {stats['jailbreak_hybrid']}/{num_results} ({stats['jailbreak_hybrid']/num_results*100:.2f}%)")
    logger.info(f"\nâš ï¸  Harm ë¶„ì„:")
    logger.info(f"  ìœ í•´ ì‘ë‹µ: {stats['harmful']}/{num_results} ({stats['harmful']/num_results*100:.2f}%)")
    logger.info(f"  í‰ê·  Harm Score: {avg_harm_score:.3f}")
    logger.info(f"\nğŸ¯ ìœ„í—˜ë„ ë¶„í¬:")
    logger.info(f"  CRITICAL: {stats['risk_critical']}")
    logger.info(f"  HIGH:     {stats['risk_high']}")
    logger.info(f"  MEDIUM:   {stats['risk_medium']}")
    logger.info(f"  LOW:      {stats['risk_low']}")
    logger.info(f"  SAFE:     {stats['risk_safe']}")
    logger.info(f"\nê²°ê³¼ ì €ì¥: {args.output_file}")


if __name__ == "__main__":
    main()

